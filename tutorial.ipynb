{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"classification.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"s0Hytkg0b_9i"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"70xKRG2ph5-F"},"source":["# Text classification\n","## Flow\n","- text -> tokenize -> padding -> model\n","\n","## Model \n","-   nn.Embedding\n","-   x = get_embedding(), \n","-   x = (len(seq), embedding_size)\n","-   z = nn.RNN(x)\n","-   nn.Linear(num_class)(z)\n","-   optimizer = Adam\n","-   softmax -> loss crossentropy -> loss backward -> optimize step -> optimize zero grad"]},{"cell_type":"code","metadata":{"id":"d0Izuzwsb9Yq"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","import pickle\n","import os\n","from tqdm import tqdm\n","from sklearn.metrics import classification_report\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data.dataloader import DataLoader\n","import numpy as np\n","import collections\n","from gensim.models import KeyedVectors\n","import torch.nn.functional as F\n","import torch.optim as optim\n","torch.manual_seed(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AO9lklhnb9Yx"},"source":["data_df = pd.read_csv('/content/gdrive/MyDrive/Machine Learning/NLP/Classification/Question_Classification_Dataset.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G85sufI5b9Yy"},"source":["data_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1LZlXm_Gb9Yy"},"source":["le = preprocessing.LabelEncoder()\n","data_texts = data_df['Questions'].to_list()\n","labels = le.fit_transform(data_df['Category0'])\n","print(data_texts[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZsDswZRb9Yz"},"source":["class_num=len(data_df['Category0'].unique())\n","print(class_num)\n","print(len(labels)) # rows =5452"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aVTRXGIzt0Ja"},"source":["# def encodingLabels(labels,class_num):\n","#   onehotLabels=[]\n","#   for label in labels:\n","#     l=[0]*class_num\n","#     l[label]=1\n","#     onehotLabels.append(l)\n","#   return onehotLabels\n","\n","# labels=encodingLabels(labels,class_num)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sF7fjNj5ek6K"},"source":["#!gzip -d '/content/gdrive/MyDrive/Machine Learning/NLP/Classification/GoogleNews-vectors-negative300.bin.gz'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSGKpJYLb9Y0"},"source":["model = KeyedVectors.load_word2vec_format('/content/gdrive/MyDrive/Machine Learning/NLP/Classification/GoogleNews-vectors-negative300.bin', binary=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DxAaJ2gSK_VD"},"source":["import re\n","\n","def getRidOfXXX(data_texts,model):\n","    new_one=[]\n","    for text in data_texts:\n","        list_tokens = []\n","        new_text=re.sub(\"[':!`\\?,\\.\\)\\(]\",'',text)\n","        \n","        word_list=new_text.split()\n","              \n","        for word in word_list:\n","          if word not in model:\n","            word_list.remove(word)\n","          else:\n","            list_tokens.append(word)\n","          \n","        new_text= ' '.join(list_tokens)\n","          \n","        new_one.append(new_text)\n","        \n","    return new_one\n","\n","data_texts_1=getRidOfXXX(data_texts,model)\n","print(data_texts_1[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzPyua52KtM1"},"source":["def getMaxLen(data_texts):\n","    max=0\n","    for text in data_texts:\n","        if len(text.split()) > max:\n","            max=len(text.split())\n","    return max\n","\n","max_len=getMaxLen(data_texts_1)\n","print(max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DQHg4DJb9Y0"},"source":["def getDic(data_texts_1,model):\n","  dic={}\n","  for text in data_texts_1:\n","    word_list=text.split()\n","    for word in word_list:\n","      if word not in model:\n","        #break\n","        print(word)\n","      elif word not in dic:\n","        dic[word]=model[word]\n","  return dic\n","\n","vocab=getDic(data_texts_1,model)\n","EMBEDDING_SIZE = vocab['what'].shape[0]\n","print(EMBEDDING_SIZE)\n","vocab['<PAD>']=np.array([0]*EMBEDDING_SIZE)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDCvvc8clDe1"},"source":["import json\n","# with open(\"vocab.json\",'w') as f:\n","#   new_vocab={}\n","#   for word in list(vocab.keys()):\n","#     new_vocab[word]=(vocab[word]).tolist()\n","#   json.dump(vocab,f,indent=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKLQu5LjQZwi"},"source":["def padding(data_texts_1,max_len):\n","    new_data=[]\n","    for text in data_texts_1:\n","        delta=max_len - len(text.split())\n","        #print(delta)\n","        new_text='<PAD> '*delta+text\n","        new_data.append(new_text)\n","    return new_data\n","\n","data_texts_2=padding(data_texts_1,max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FODGNZmGNfII"},"source":["#print(data_texts_2[0])\n","\n","def encodingData(data_texts_2,vocab):\n","  data=[]\n","  for text in data_texts_2:\n","    text_to_vec=[]\n","    for word in text.split():\n","      text_to_vec.append(vocab[word])\n","    data.append(text_to_vec)\n","  return np.array(data)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4YshPUvvBZF"},"source":["data=torch.from_numpy(encodingData(data_texts_2,vocab))\n","targets=torch.from_numpy(np.array(labels))\n","print(targets.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3CbYBUfmmifj"},"source":["#X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.2, random_state=0)\n","X_train_val, X_test, y_train_val, y_test = train_test_split(data, targets, test_size=0.2, random_state=2000)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.4, random_state=2000)      \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WU1q_AaHjxr4"},"source":["class makeDataset(Dataset):\n","    def __init__(self,data,labels):\n","        self.data=data\n","        self.labels=labels\n","        self.n_samples=data.shape[0]\n","        \n","    def __getitem__(self,idx):\n","        return self.data[idx],self.labels[idx]\n","\n","    \n","    def __len__(self):\n","        return self.n_samples\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"__DLqe5fh3V4"},"source":["class RNN(nn.Module):\n","    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n","        super(RNN, self).__init__()\n","        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, nonlinearity='tanh')\n","        self.linear1 = nn.Linear(in_features=hidden_dim, out_features=output_size)\n","        self.classifier = nn.Softmax()\n","\n","    def forward(self, X):\n","        out,hidden = self.rnn(X)\n","        #print(out.size())\n","        out = out[:, -1, :]\n","        out = self.linear1(out)\n","        out = self.classifier(out)\n","\n","        return out\n","\n","\n","\n","class QuestionClassifier(nn.Module):\n","    def __init__(self,n_classes,pretrained_model_name='bert-base-uncased'):\n","        super(QuestionClassifier,self).__init__()\n","        self.bert=BertModel.from_pretrained(pretrained_model_name)\n","        self.dense=nn.Linear(self.bert.config.hidden_size,n_classes)\n","        \n","    def forward(self,input_ids):\n","        hidden_states,pooled_output=self.bert(input_ids=input_ids)\n","        sequence_output_cls=hidden_states[0,:,0]\n","        x=self.dropout(sequence_output_cls)\n","        x=self.dense(x)\n","        x=get_activation('tanh')(x)\n","        x=self.dropout(x)\n","        x=self.out_proj(x)\n","        return x\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruDKUFJFijN7"},"source":["from torch.optim import Adam\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_score, f1_score, recall_score\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZUKp7446rPs-"},"source":["BATCH_SIZE=20\n","train_dataset = makeDataset(X_train, y_train)\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n","val_dataset = makeDataset(X_val, y_val)\n","val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n","test_dataset = makeDataset(X_test, y_test)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n","model = RNN(input_size=300, output_size=6, hidden_dim=64, n_layers=1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6B5iWF-Cb9Y3"},"source":["\n","lr = 0.0001\n","N_EPOCHS = 2000\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = Adam(model.parameters(), lr=lr)\n","MODEL_SAVE_PATH = './rnn_model.pt'\n","# train\n","train_losses = []\n","val_losses = []\n","best_val_loss = 1000\n","\n","for epoch in tqdm(range(N_EPOCHS)):\n","    print('\\nEpoch {}: '.format(epoch + 1))\n","\n","    train_loss = []\n","    for X_train_batch, y_train_batch in train_dataloader:\n","        out = model(X_train_batch.float())\n","        loss = loss_fn(out, y_train_batch)\n","        train_loss.append(loss.item())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    train_l=sum(train_loss) / len(train_loss)\n","    train_losses.append(train_l)\n","    print(train_l)\n","\n","    val_loss = []\n","    for X_val_batch, y_val_batch in val_dataloader:\n","        out = model(X_val_batch.float())\n","        loss = loss_fn(out, y_val_batch)\n","        train_loss.append(loss.item())\n","    val_losses.append(sum(train_loss) / len(train_loss))\n","    if best_val_loss > val_losses[-1]:\n","        best_val_loss = val_losses[-1]\n","        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n","\n","print(\"Train loss: \", train_losses)\n","print(\"Validation loss: \", val_losses)\n","\n","x = np.arange(len(train_losses))\n","fig, ax = plt.subplots()\n","ax.plot(x, train_losses, label='Train loss')\n","ax.plot(x, val_losses, label='Validation loss')\n","ax.legend()\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5v0sBmZGrQY4"},"source":["\n","model.state_dict(torch.load(MODEL_SAVE_PATH))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MX09ebknk9_G"},"source":["print(\"Test results: \")\n","for X_test, y_test in test_dataloader:\n","    pred = torch.argmax(model(X_test.float()), dim=1)\n","    print(\"Test precision: {}\".format(precision_score(y_test, pred, average='weighted')))\n","    print(\"Test recall: {}\".format(recall_score(y_test, pred, average='weighted')))\n","    print(\"Test F1-score: {}\".format(f1_score(y_test, pred, average='weighted')))"],"execution_count":null,"outputs":[]}]}